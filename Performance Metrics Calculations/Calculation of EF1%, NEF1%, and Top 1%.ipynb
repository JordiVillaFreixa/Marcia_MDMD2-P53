{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419aea76-f703-4293-9815-0ed5cc5f7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('SVM/SVM-run9-new.csv')  # Path to the output file from each run (VSBS) of each ML model saved as .csv in its respective folder\n",
    "\n",
    "# Sort by the 'Active_Prob' column in descending order\n",
    "df_sorted = df.sort_values(by='Active_Prob', ascending=False)\n",
    "\n",
    "# Save the sorted file (optional)\n",
    "df_sorted.to_csv('SVM/AO-SVM-9.csv', index=False)  # Sorted output file for each ML model and run in its corresponding folder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize a list to store the results of each model\n",
    "all_results = []\n",
    "\n",
    "# Define the models and their respective folders\n",
    "models = {\n",
    "    'XGB': 'XGB',\n",
    "    'SVM': 'SVM',\n",
    "    'DNN': 'DNN',\n",
    "    'ANN': 'ANN',\n",
    "    'RF': 'RF'\n",
    "}\n",
    "\n",
    "# Read and process the files of each model in its corresponding folder\n",
    "for model, folder in models.items():\n",
    "    for i in range(1, 11):\n",
    "        # Create the file path for each model run\n",
    "        file_name = f'AO-{model}-{i}.csv'  # All files are in the same folder. AO stands for \"ordered file\", model is the ML model name, and i is the run number for training and testing\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        \n",
    "        # Check if the file exists before reading it\n",
    "        if os.path.exists(file_path):\n",
    "            df_sorted = pd.read_csv(file_path)\n",
    "            \n",
    "            # Extract the relevant columns\n",
    "            real_classes = df_sorted['Real_Class'].values\n",
    "            predicted_classes = df_sorted['Predicted_Class'].values\n",
    "            \n",
    "            # Count how many times Real_Class and Predicted_Class both are \"Active\"\n",
    "            correct_active_count = np.sum((real_classes == 'Active') & (predicted_classes == 'Active'))\n",
    "            \n",
    "            # Count how many times Real_Class and Predicted_Class match in the top 1%\n",
    "            n_total = len(real_classes)\n",
    "            n_top_1_percent = max(1, n_total // 100)\n",
    "            \n",
    "            # Extract the first n_top_1_percent rows for the top 1%\n",
    "            top_1_real_classes = real_classes[:n_top_1_percent]\n",
    "            top_1_predicted_classes = predicted_classes[:n_top_1_percent]\n",
    "            \n",
    "            # Count how many times Real_Class and Predicted_Class match in the top 1%\n",
    "            correct_active_top_1_percent = np.sum((top_1_real_classes == 'Active') & (top_1_predicted_classes == 'Active'))\n",
    "            \n",
    "            # Calculate EF1%\n",
    "            ef1_percent = (correct_active_top_1_percent / correct_active_count) * 100 if correct_active_count > 0 else 0\n",
    "            \n",
    "            # Calculate EF1% max\n",
    "            if correct_active_count <= n_top_1_percent:\n",
    "                ef1_max = 100\n",
    "            else:\n",
    "                ef1_max = (n_top_1_percent / correct_active_count) * 100\n",
    "            \n",
    "            # Calculate NEF1%\n",
    "            nef1_percent = ef1_percent / ef1_max if ef1_max > 0 else 0\n",
    "            \n",
    "            # Store the results in a list\n",
    "            all_results.append({\n",
    "                'Model': model,\n",
    "                'Run': i,\n",
    "                'EF1%': ef1_percent,\n",
    "                'NEF1%': nef1_percent,\n",
    "                'EF1% max': ef1_max,\n",
    "                'Total Active Correct': correct_active_count,\n",
    "                'Total Active in Top 1%': correct_active_top_1_percent\n",
    "            })\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Convert the list of results into a DataFrame\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df_results.to_csv('model_results.csv', index=False)\n",
    "\n",
    "# Calculate the average of each metric by model\n",
    "df_avg_results = df_results.groupby('Model').agg({\n",
    "    'EF1%': 'mean',\n",
    "    'NEF1%': 'mean',\n",
    "    'EF1% max': 'mean',\n",
    "    'Total Active Correct': 'mean',\n",
    "    'Total Active in Top 1%': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Save the averages to a CSV file\n",
    "df_avg_results.to_csv('model_avg_results.csv', index=False)  # File containing the results: Model, Run, EF1%, NEF1%, EF1% max, Total Active Correct, and Total Active in Top 1%\n",
    "\n",
    "# Show the generated DataFrames\n",
    "print(\"Results per model and run:\")\n",
    "print(df_results)\n",
    "print(\"\\nAverage results per model:\")\n",
    "print(df_avg_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
